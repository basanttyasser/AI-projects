{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b863136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:52:36.626744Z",
     "start_time": "2023-07-16T21:52:35.773751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20100\\AppData\\Local\\Temp\\ipykernel_11572\\4210854531.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv('data.txt',delimiter='\\;', header=None, names = ['comment', 'emotion'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joy         5362\n",
       "sadness     4666\n",
       "love        2692\n",
       "anger       2159\n",
       "fear        1937\n",
       "surprise    1905\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.txt',delimiter='\\;', header=None, names = ['comment', 'emotion'])\n",
    "# In feature engineering we will create 3 features (text_len),(punctuation_perc), and (capital_perc)\n",
    "import string\n",
    "#adding feature (text_len)\n",
    "data['body_len'] = data['comment'].apply(lambda i : len(i)-i.count(\" \"))\n",
    "data.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0838a7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:52:54.064193Z",
     "start_time": "2023-07-16T21:52:53.885230Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub('\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the content column\n",
    "data['comment'] = data['comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89ff514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:53:09.594313Z",
     "start_time": "2023-07-16T21:53:06.357706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[didnt, feel, humiliated]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[go, feeling, hopeless, damned, hopeful, aroun...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[im, grabbing, minute, post, feel, greedy, wrong]</td>\n",
       "      <td>anger</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ever, feeling, nostalgic, fireplace, know, st...</td>\n",
       "      <td>love</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[feeling, grouchy]</td>\n",
       "      <td>anger</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  emotion  body_len\n",
       "0                          [didnt, feel, humiliated]  sadness        20\n",
       "1  [go, feeling, hopeless, damned, hopeful, aroun...  sadness        88\n",
       "2  [im, grabbing, minute, post, feel, greedy, wrong]    anger        39\n",
       "3  [ever, feeling, nostalgic, fireplace, know, st...     love        75\n",
       "4                                 [feeling, grouchy]    anger        17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization & Removing 'stop words' for training\n",
    "from nltk.tokenize import word_tokenize\n",
    "filtered_sentence_arr=[]\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "for example_sent in data['comment']:\n",
    "    word_tokens = word_tokenize(example_sent)\n",
    "    # converts the words in word_tokens to lower case and then checks whether\n",
    "    #they are present in stop_words or not\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w.lower())\n",
    "    filtered_sentence_arr.append(filtered_sentence)\n",
    "data['comment']=filtered_sentence_arr\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bf4dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:53:24.158200Z",
     "start_time": "2023-07-16T21:53:22.574827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>love</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  emotion  body_len\n",
       "0                              didnt feel humiliated  sadness        20\n",
       "1  go feeling hopeless damned hopeful around some...  sadness        88\n",
       "2          im grabbing minute post feel greedy wrong    anger        39\n",
       "3  ever feeling nostalgic fireplace know still pr...     love        75\n",
       "4                                    feeling grouchy    anger        17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second (Stemming) & (lemmitization) for training\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "arr1=[]\n",
    "arr2=[]\n",
    "for i in data['comment']:\n",
    "    arr1=[]\n",
    "    for j in i:\n",
    "        a=lemmatizer.lemmatize(j)\n",
    "        arr1.append(a)\n",
    "    arr2.append(\" \".join(arr1))\n",
    "data['comment']=arr2\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612f0a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:54:40.469556Z",
     "start_time": "2023-07-16T21:54:38.894260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16848x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 130200 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting TF-IDF parameters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
    "xx = tfidf.fit_transform(data['comment'])\n",
    "\n",
    "#Training & Testing Encoding\n",
    "y = data.emotion.replace(to_replace=['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'], value=[0,1,2,3,4,5])\n",
    "x=pd.concat([data['body_len'],pd.DataFrame(xx)],axis=1)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# Extracting TF-IDF parameters\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578d6606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:55:20.637642Z",
     "start_time": "2023-07-16T21:55:20.621681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes tfidf accuracy 0.6705819540843566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "y_pred = nb.predict(x_test)\n",
    "acc_naive=accuracy_score(y_pred, y_test)\n",
    "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17bf5c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:56:45.094454Z",
     "start_time": "2023-07-16T21:55:44.943025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest tfidf accuracy 0.7346502936465563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "acc_random=accuracy_score(y_pred, y_test)\n",
    "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e1bbd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:56:45.513945Z",
     "start_time": "2023-07-16T21:56:45.188477Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['comment'], y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n",
    "# Extracting Count Vectors Parameters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(data['comment'])\n",
    "X_train_count =  count_vect.transform(x_train)\n",
    "X_val_count =  count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1398c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:56:45.811372Z",
     "start_time": "2023-07-16T21:56:45.609561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvm using count vectors accuracy 0.9113721302722905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "acc_lsvm=accuracy_score(y_pred, y_test)\n",
    "x_acc=acc_lsvm\n",
    "print('lsvm using count vectors accuracy %s' % x_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad86dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:56:50.848805Z",
     "start_time": "2023-07-16T21:56:50.842821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvm using count vectors accuracy 0.9113721302722905\n"
     ]
    }
   ],
   "source": [
    "print('lsvm using count vectors accuracy %s' % x_acc)\n",
    "arr_name={\n",
    "0:'sadness',\n",
    "1:'anger',\n",
    "2:'love',\n",
    "3:'surprise',\n",
    "4:'fear',\n",
    "5:'joy',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed566ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T21:58:57.325529Z",
     "start_time": "2023-07-16T21:57:35.155944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter end to end or enter example  :are you kidding me\n",
      "joy\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "#example_sent='im grabbing a minute to post i feel greedy wrong'\n",
    "example_sent=input(\"enter end to end or enter example  :\")\n",
    "def word_test(example_sent):\n",
    "    #Tokenization & Removing 'stop words' for training\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_tokens = word_tokenize(example_sent)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(lemmatizer.lemmatize(w.lower()))\n",
    "            #a=lemmatizer.lemmatize(j)\n",
    "    filtered_sentence=\" \".join(filtered_sentence)\n",
    "    filtered_sentence=[filtered_sentence]\n",
    "    example_sent_count =  count_vect.transform(filtered_sentence)\n",
    "    y_pred = lsvm.predict(example_sent_count)\n",
    "    \n",
    "    x= arr_name[y_pred[0]]\n",
    "    print(x)\n",
    "    return x\n",
    "y_pred1=''\n",
    "y_pred1=word_test(example_sent)\n",
    "print(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eabb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
